# ============================== #
#            IMPORTS             #
# ============================== #
# API ë¶ˆëŸ¬ì˜¤ê¸°
import os
from dotenv import load_dotenv

# ì›ë³¸ íŒŒì¼ ì •ë¦¬
import pandas as pd
from langchain_community.document_loaders import PyPDFLoader, UnstructuredHTMLLoader, Docx2txtLoader
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_core.documents import Document

import datetime as dt
from zoneinfo import ZoneInfo
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.retrievers import ContextualCompressionRetriever
from langchain_cohere import CohereRerank
from langchain_community.document_transformers import LongContextReorder
from langchain.retrievers.document_compressors import DocumentCompressorPipeline

# streamlit
import streamlit as st
from streamlit_lottie import st_lottie
import requests

# ============================== #
#        ENV & CONSTANTS         #
# ============================== #
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
COHERE_API_KEY = os.getenv("COHERE_API_KEY")
KST = ZoneInfo("Asia/Seoul")

# ============================== #
#      SIDEBAR (ì„  ì •ì˜ í•„ìš”)     #
# ============================== #
with st.sidebar:
    st.title("ğŸ€ëª¨ë‘ì˜ ì—°êµ¬ì†Œ")

    # ë‚ ì§œ ì„ íƒ (UIëŠ” ì‚¬ì´ë“œë°”ì— ë Œë”ë˜ì§€ë§Œ, ì½”ë“œ ìˆœì„œëŠ” ìƒë‹¨ì´ì–´ë„ OK)
    st.markdown("---")
    st.header("ğŸ—“ï¸ ë‚ ì§œ ì„ íƒ")
    today_default = dt.date.today()
    selected_date = st.date_input(
        "ì›í•˜ëŠ” ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        value=today_default,
        min_value=dt.date(2023, 1, 1),
        max_value=dt.date(2026, 12, 31),
        key="sidebar_date"
    )
    st.markdown("---")
    st.info(f"ì˜¤ëŠ˜ì€: **{selected_date}** ì…ë‹ˆë‹¤.")

    # ê´€ë ¨ ì‚¬ì´íŠ¸
    st.markdown("---")
    st.header("ğŸ”—ê´€ë ¨ ì‚¬ì´íŠ¸")
    st.link_button("ëª¨ë‘ì˜ ì—°êµ¬ì†Œ í™ˆí˜ì´ì§€", "https://modulabs.co.kr")
    st.link_button("ë°ì‹¸ 5ê¸° ë…¸ì…˜ ì›Œí¬ìŠ¤í˜ì´ìŠ¤", "https://www.notion.so/New-5-25-07-07-26-01-08-New-23f2d25db62480828becc399aaa41877")
    st.link_button("ë°ì‹¸ 5ê¸° ZEP", "https://zep.us/play/8l5Vdo")
    st.link_button("LMS í™ˆí˜ì´ì§€", "https://lms.aiffel.io/")

    # ì²¨ë¶€íŒŒì¼
    st.markdown("---")
    st.header("ğŸ“„ì²¨ë¶€íŒŒì¼")
    try:
        with open(r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\word\íœ´ê°€ì‹ ì²­ì„œ(ë°ì‹¸_5ê¸°).docx", 'rb') as file:
            st.download_button(
                label='íœ´ê°€ì‹ ì²­ì„œ ë‹¤ìš´ë¡œë“œ',
                data=file,
                file_name='íœ´ê°€ì‹ ì²­ì„œ.docx',
                mime='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
            )
    except FileNotFoundError:
        st.warning(r"ì²¨ë¶€íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: C:\Users\user\Desktop\MODULABS\LangchainThon\Data\word\íœ´ê°€ì‹ ì²­ì„œ(ë°ì‹¸_5ê¸°).docx")

# ============================== #
#         TEXT SPLITTER          #
# ============================== #
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=128,
    separators=["\n\n", "\n", " ", ""]
)

# ============================== #
#           LOAD FILES           #
# ============================== #
# PDF
PDF_PATH = r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\pdf\ëª¨ë‘ì—° ë¸Œëœë”©ë¶ ì •ë¦¬.pdf"
docs_pdf = []
try:
    loader_pdf = PyPDFLoader(PDF_PATH)
    pages_pdf = loader_pdf.load()
    for d in pages_pdf:
        d.metadata["source_type"] = "pdf"
        d.metadata["source"] = os.path.basename(PDF_PATH)
    docs_pdf = text_splitter.split_documents(pages_pdf)
except Exception as e:
    st.error(f"PDF ë¡œë”© ì‹¤íŒ¨: {e}")

# HTML
HTML_PATH = [
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\LMS oops í•´ê²°ë²•.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\LMS ì•„ì´í  ë…¸íŠ¸ë¶ì´ ì•„ë‹™ë‹ˆë‹¤ ì—ëŸ¬.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\LMS ì´ìš©ì‹œ ë°œìƒí•˜ëŠ” ë¬¸ì œ í•´ê²°ë²•.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\êµìœ¡ê³¼ì • ì¤‘ ì·¨ì—… ì‹œ.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ë°ì‹¸ 5ê¸° í›ˆë ¨ ì •ë³´.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ìˆ˜ê°• ì¤‘ ê³ ìš© í˜•íƒœ ê´€ë ¨ ì•ˆë‚´.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ìŠ¤í„°ë””ë¥¼ ë§Œë“¤ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ì˜¤í”„ë‹ ì¥ì†Œì™€ í´ë¡œì§• ì¥ì†Œê°€ ë‹¤ë¦…ë‹ˆë‹¤.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ì¸í„°ë„·ì´ ë¶ˆì•ˆì •í•˜ì—¬ ì¶œê²° QRì„ ì œëŒ€ë¡œ ì°ì§€ ëª»í•˜ì˜€ìŠµë‹ˆë‹¤.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ì œì  ê°€ì´ë“œ.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\ì¶œê²° ë° ê³µê°€ì— ëŒ€í•˜ì—¬.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\íˆ´ ì„¸íŒ….html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\í›ˆë ¨ ì¥ë ¤ê¸ˆ ì§€ê¸‰ í™•ì¸.html",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\html\í›ˆë ¨ ì°¸ì—¬ ê·œì¹™.html",
]
docs_html = []
try:
    html_list = []
    for path in HTML_PATH:
        loader_html = UnstructuredHTMLLoader(path)
        pages_html = loader_html.load()
        for d in pages_html:
            d.metadata["source_type"] = "html"
            d.metadata["source"] = os.path.basename(path)
        html_list.extend(pages_html)
    docs_html = text_splitter.split_documents(html_list)
except Exception as e:
    st.error(f"HTML ë¡œë”© ì‹¤íŒ¨: {e}")

# WORD
WORD_PATH = r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\word\íœ´ê°€ì‹ ì²­ì„œ(ë°ì‹¸_5ê¸°).docx"
docs_word = []
try:
    loader_word = Docx2txtLoader(WORD_PATH)
    pages_word = loader_word.load()
    for d in pages_word:
        d.metadata["source_type"] = "word"
        d.metadata["source"] = os.path.basename(WORD_PATH)
    docs_word = text_splitter.split_documents(pages_word)
except Exception as e:
    st.error(f"WORD ë¡œë”© ì‹¤íŒ¨: {e}")

# CSV (ë™ë£Œ/ìš´ì˜ì§„)
CSV_PATH = [
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\csv\ë°ì‹¸ 5ê¸° ë™ë£Œë“¤.csv",
    r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\csv\ë°ì‹¸ 5ê¸° ìš´ì˜ì§„.csv"
]
docs_csv = []
try:
    csv_list = []
    for path in CSV_PATH:
        loader_csv = CSVLoader(path, encoding='cp949')
        pages_csv = loader_csv.load()
        for d in pages_csv:
            d.metadata["source_type"] = "csv"
            d.metadata["source"] = os.path.basename(path)
        csv_list.extend(pages_csv)
    docs_csv = text_splitter.split_documents(csv_list)
except Exception as e:
    st.error(f"CSV ë¡œë”© ì‹¤íŒ¨: {e}")

# CSV (ì¼ì •í‘œ â†’ í•™ìƒë³„ ê·¸ë£¹ ë¬¸ì„œ)
def create_grouped_documents(csv_path: str) -> list[Document]:
    try:
        df = pd.read_csv(csv_path, encoding='cp949')
    except Exception as e:
        st.error(f"ì¼ì •í‘œ CSV ë¡œë”© ì‹¤íŒ¨: {e}")
        return []

    required_cols = ['ì´ë¦„', 'ì‚¬ìœ ', 'ë‚ ì§œ', 'ë¶€ì¬ì‹œê°„', 'ìƒíƒœ']
    if not all(col in df.columns for col in required_cols):
        st.error(f"í•„ìš” ì»¬ëŸ¼ ëˆ„ë½: {required_cols}")
        return []

    df = df[required_cols].fillna('')
    documents = []
    grouped = df.groupby('ì´ë¦„')

    for name, group_df in grouped:
        record_strings = []
        for _, row in group_df.iterrows():
            record = (
                f"ì‚¬ìœ : {row['ì‚¬ìœ ']}, "
                f"ë‚ ì§œ: {row['ë‚ ì§œ']}, "
                f"ìƒíƒœ: {row['ìƒíƒœ']}, "
                f"ë¶€ì¬ì‹œê°„: {row['ë¶€ì¬ì‹œê°„']}"
            )
            record_strings.append(record)

        full_records_text = "\n".join(record_strings)
        document = Document(
            page_content=f"í•™ìƒ ì´ë¦„: {name}\n\n--- ì „ì²´ ì¶œê²° ê¸°ë¡ ì‹œì‘ ---\n{full_records_text}",
            metadata={'í•™ìƒì´ë¦„': name, 'ì´ê¸°ë¡ìˆ˜': len(group_df)}
        )
        documents.append(document)
    return documents

docs_attendance = []
attendance_path = r"C:\Users\user\Desktop\MODULABS\LangchainThon\Data\csv\ë°ì‹¸ 5ê¸° ì¼ì •í‘œ.csv"
attendance_documents = create_grouped_documents(attendance_path)
docs_attendance = text_splitter.split_documents(attendance_documents) if attendance_documents else []

# ============================== #
#           VECTOR DB            #
# ============================== #
# ì„ë² ë”© ë° ì¸ë±ì‹±
vectorstore = Chroma.from_documents(docs_html, OpenAIEmbeddings(model='text-embedding-3-large'))
if docs_word: vectorstore.add_documents(docs_word)
if docs_csv: vectorstore.add_documents(docs_csv)
if docs_pdf: vectorstore.add_documents(docs_pdf)
if docs_attendance: vectorstore.add_documents(docs_attendance)

# ============================== #
#            RETRIEVER           #
# ============================== #

# Reranking ì´ì „ base 
base_retriever = vectorstore.as_retriever(
    search_type="mmr", 
    search_kwargs={"lambda_mult": 0.4, "fetch_k": 96, "k": 48}
)

# Rerank
reranker = CohereRerank(
    model="rerank-multilingual-v3.0",    
    top_n=10                              
)

# Reorder
reorder = LongContextReorder()

# Rerank + Reorder
compressor = DocumentCompressorPipeline(transformers=[reranker, reorder])

upgraded_retriever = ContextualCompressionRetriever(
    base_retriever=base_retriever,
    base_compressor=compressor            
)

# ============================== #
#               LLM              #
# ============================== #
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# ============================== #
#           PROMPTS/CHAINS       #
# ============================== #
contextualize_q_system_prompt = """
ì´ì „ ëŒ€í™”ê°€ ìˆë‹¤ë©´ ì°¸ê³ í•˜ì—¬,
ì‚¬ìš©ìì˜ ìµœì‹  ì§ˆë¬¸ì„ ë…ë¦½ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥í•œ í•œ ë¬¸ì¥ìœ¼ë¡œ ë°”ê¿”ì£¼ì„¸ìš”.
ë‹µë³€í•˜ì§€ ë§ê³  ì§ˆë¬¸ë§Œ ì¬ì‘ì„±í•˜ì„¸ìš”.
"""
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

history_aware_retriever = create_history_aware_retriever(
    llm,
    upgraded_retriever,
    contextualize_q_prompt
)

qa_system_prompt = """
ë‹¹ì‹ ì€ 'ëª¨ë‘ì˜ì—°êµ¬ì†Œ(ëª¨ë‘ì—°)' ìˆ˜ê°•ìƒë“¤ì˜ ë¹„ì„œì…ë‹ˆë‹¤.

í˜„ì¬ ì‹œê°„ì€ {today} (KST)ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ 'ì–´ì œ, ë‚´ì¼' ë“±ì˜ í‘œí˜„ì€ {today}ë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒŒì•…í•˜ì„¸ìš”.
ì˜¤ëŠ˜ì˜ ë‚ ì§œ/ìš”ì¼ì€ {today_ko} / {weekday_ko} ì…ë‹ˆë‹¤. ë‚ ì§œ ë° ìš”ì¼ ê´€ë ¨ ì§ˆë¬¸ì—ëŠ” ì¶”ë¡ í•˜ì§€ ë§ê³  ë°˜ë“œì‹œ ì´ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.

ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê·¼ê±°ë¡œ ë‹µí•˜ì„¸ìš”. ê·¼ê±°ê°€ ì—†ìœ¼ë©´ 'ì •ë³´ê°€ ëª…í™•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìš´ì˜ë§¤ë‹ˆì €ë‹˜ì´ë‚˜ í¼ì‹¤ë‹˜ê»˜ ë¬¸ì˜í•´ì£¼ì„¸ìš”.'ë¼ê³ ë§Œ ëŒ€ë‹µí•˜ì„¸ìš”.
ì‚¬ìš©ì ì…ë ¥ì— í¬í•¨ëœ ì‚¬ì‹¤ì€ ê·¼ê±°ë¡œ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

í›ˆë ¨ì¥ë ¤ê¸ˆì˜ ê²½ìš° ì£¼ì–´ì§„ ë‹¨ìœ„ ê¸°ê°„ ì¼ìˆ˜ì˜ 80%ì´ìƒì„ ì¶œì„í•´ì•¼ë§Œ ê¸ˆì•¡ì´ ì§€ê¸‰ë¨ì„ ëª…ì‹¬í•˜ì„¸ìš”. 
ìµœëŒ€ 3ë¬¸ì¥ìœ¼ë¡œ ì§§ê²Œ ë‹µë³€í•˜ì„¸ìš”.

{context}
"""
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
rag_core_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# ============================== #
#       SESSION / HISTORY        #
# ============================== #
if "lc_store" not in st.session_state:
    st.session_state["lc_store"] = {}  

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    store = st.session_state["lc_store"]  
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

conversational_rag_chain = RunnableWithMessageHistory(
    rag_core_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer",
)

# ============================== #
#           LOTTIE + UI          #
# ============================== #
def load_lottie_url(url):
    r = requests.get(url)
    if r.status_code != 200:
        return None
    return r.json()

snow_lottie_url = "https://assets2.lottiefiles.com/packages/lf20_1pxqjqps.json"
snow_animation = load_lottie_url(snow_lottie_url)
if snow_animation:
    st_lottie(snow_animation, speed=1, reverse=False, loop=True, quality="high", height=500, width=800, key="snow")

st.markdown(
    """
<div style="text-align: center;">
    <p style="font-size:25px;">
        ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ëª¨ë‘ë´‡ì…ë‹ˆë‹¤.<br>ì¦ê±°ìš´ ëª¨ë‘ì—° ìƒí™œì„ ìœ„í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.ğŸ˜Š
    </p>
</div>
""",
    unsafe_allow_html=True
)

if "session_id" not in st.session_state:
    st.session_state["session_id"] = "default"
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]

for msg in st.session_state["messages"]:
    st.chat_message(msg["role"]).write(msg["content"])

# ============================== #
#            CHAT LOOP           #
# ============================== #
if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)"):
    st.session_state["messages"].append({"role": "user", "content": prompt_message})
    st.chat_message("user").write(prompt_message)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            # ì‚¬ì´ë“œë°”ì—ì„œ ê³ ë¥¸ ë‚ ì§œë¥¼ todayë¡œ ë™ì  ì£¼ì… (KST 00:00:00ë¡œ ê³ ì •)
            today_override = f"{selected_date} 00:00:00"

            # selected_dateê°€ ì´ë¯¸ date ê°ì²´ë¼ê³  ê°€ì •
            weekday_names = ["ì›”ìš”ì¼","í™”ìš”ì¼","ìˆ˜ìš”ì¼","ëª©ìš”ì¼","ê¸ˆìš”ì¼","í† ìš”ì¼","ì¼ìš”ì¼"]
            weekday_ko = weekday_names[selected_date.weekday()]
            today_ko = f"{selected_date.year}ë…„ {selected_date.month}ì›” {selected_date.day}ì¼"

            resp = conversational_rag_chain.invoke(
                {
                    "input": prompt_message,
                    "today": today_override,
                    "today_ko": today_ko,
                    "weekday_ko": weekday_ko,
                },
                config={"configurable": {"session_id": st.session_state["session_id"]}},
            )

            answer = resp if isinstance(resp, str) else resp.get("answer", "")
            st.write(answer)
            st.session_state["messages"].append({"role": "assistant", "content": answer})
